{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text-Sentiment-Analysis-using-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuluW8071/Text-Sentiment-Analysis/blob/main/Text_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-taomIznYua"
      },
      "source": [
        "## 1. Download and Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rFh4JzLQSHR"
      },
      "source": [
        "The dataset that the following script will download is a combination of the `Yelp Polarity Dataset` and the `IMDb Movie Dataset`. The `Yelp Polarity Dataset` has been preprocessed by selecting specific columns to create a dataset suitable for **sentiment analysis**. This preprocessed dataset has been merged with the `IMDb Movie Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrooSaLbAX4j",
        "outputId": "92b78a37-7b30-4b95-e3a1-ae05dae0b38e"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "file_url = 'https://drive.google.com/uc?id=1Jp3D5gdxGrwa5dHbr4p-pECrD8wi7vik'\n",
        "file_name = 'sentiment_dataset.zip'\n",
        "\n",
        "# Download the file from Google Drive\n",
        "gdown.download(file_url, file_name, quiet=False)\n",
        "extract_dir = './dataset'\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Remove the zip file after extraction\n",
        "os.remove(file_name)\n",
        "print(\"Files extracted successfully to:\", extract_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PHvIkDPkkoc2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lQLGbbvAm-hs",
        "outputId": "eba6a748-6f49-4163-ba0a-9e2f797805c1"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"dataset/sentiment_combined.csv\")\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00gmM1X-nbwW"
      },
      "source": [
        "## 2. Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_PxnoXPknrMg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "F-Vxh1WPnGeJ",
        "outputId": "03b6d69d-399f-4cc0-94a5-1ac78bdb5547"
      },
      "outputs": [],
      "source": [
        "X, y = df['review'].values,df['sentiment'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size = 0.1)\n",
        "print(f'train data shape: {X_train.shape}')\n",
        "print(f'test data shape: {X_test.shape}')\n",
        "\n",
        "\n",
        "val = pd.Series(y_train).value_counts()\n",
        "sns.barplot(x = np.array(['negative','positive']),\n",
        "            y = val.values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0aQLjLNoh58"
      },
      "source": [
        "## 3. Text Pre-Processing\n",
        "- Cleaning up the text data by removing punctuation, extra spaces, and numbers.\n",
        "- Transform sentences into individual words, remove common words (known as \"stop words\")\n",
        "- Keeping track of the 1000 most frequently used words in the dataset\n",
        "- Assigning tracked words a unique identifier, forming dictionary for one-hot encoding\n",
        "- Ensuring all sequences have the same length by padding or truncating them as needed by 500 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTeEZdMSpBeq",
        "outputId": "5a0be59d-dba4-4b5d-9c0e-05b8aad92615"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from collections import Counter\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "huj_ZpwyoM_F"
      },
      "outputs": [],
      "source": [
        "# Precompile regular expressions for faster pre processing [improved by 50% in preprocess]\n",
        "non_word_chars_pattern = re.compile(r\"[^\\w\\s]\")\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "digits_pattern = re.compile(r\"\\d\")\n",
        "username_pattern = re.compile(r\"@([^\\s]+)\")\n",
        "hashtags_pattern = re.compile(r\"#\\d+\")\n",
        "\n",
        "def preprocess_string(s):\n",
        "    # Remove all non-word characters (everything except numbers and letters)\n",
        "    s = non_word_chars_pattern.sub('', s)\n",
        "    # Replace all runs of whitespaces with single space\n",
        "    s = whitespace_pattern.sub(' ', s)\n",
        "    # Replace digits with no space\n",
        "    s = digits_pattern.sub('', s)\n",
        "    # Replace usernames with no space\n",
        "    s = username_pattern.sub('', s)\n",
        "    # Replace hashtags with no space\n",
        "    s = hashtags_pattern.sub('', s)\n",
        "    # Replace specific characters\n",
        "    s = s.replace(\"https\", \"\")\n",
        "    s = s.replace(\"http\", \"\")\n",
        "    s = s.replace(\"rt\", \"\")\n",
        "    s = s.replace(\"-\", \"\")\n",
        "    # Replace <br /> with a space\n",
        "    s = s.replace(\"<br />\", \" \")\n",
        "    # Replace newline character with empty string\n",
        "    s = s.replace(\"\\n\", \"\")\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FzaCKiCFvafI"
      },
      "outputs": [],
      "source": [
        "# Create a Tokenize including padding Function\n",
        "def tokenize(X_train, y_train, X_val, y_val, seq_len):\n",
        "    word_list = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for sent in tqdm(X_train, desc=\"Preprocessing train data\"):\n",
        "        words = [preprocess_string(word) for word in sent.lower().split()]\n",
        "        word_list.extend([word for word in words if word not in stop_words and word != ''])\n",
        "\n",
        "    corpus = Counter(word_list)\n",
        "    # Sorting on the basis of most common words\n",
        "    corpus_ = sorted(corpus, key=corpus.get, reverse=True)[:1000]\n",
        "    # Creating a dictionary\n",
        "    onehot_dict = {w: i + 1 for i, w in enumerate(corpus_)}\n",
        "\n",
        "    # Tokenize and pad sequences\n",
        "    final_list_train = [[onehot_dict.get(preprocess_string(word), 0) for word in sent.lower().split() if preprocess_string(word) in onehot_dict]\n",
        "                        for sent in tqdm(X_train, desc=\"Tokenizing and padding train data\")]\n",
        "    final_list_train = [padded_tokens[-seq_len:] if len(padded_tokens) > seq_len else [0] * (seq_len - len(padded_tokens)) + padded_tokens\n",
        "                        for padded_tokens in final_list_train]\n",
        "\n",
        "    final_list_test = [[onehot_dict.get(preprocess_string(word), 0) for word in sent.lower().split() if preprocess_string(word) in onehot_dict]\n",
        "                       for sent in tqdm(X_val, desc=\"Tokenizing and padding validation data\")]\n",
        "    final_list_test = [padded_tokens[-seq_len:] if len(padded_tokens) > seq_len\n",
        "                       else [0] * (seq_len - len(padded_tokens)) + padded_tokens\n",
        "                       for padded_tokens in final_list_test]\n",
        "\n",
        "    encoded_train = np.array([1 if label == 'positive' else 0 for label in y_train])\n",
        "    encoded_test = np.array([1 if label == 'positive' else 0 for label in y_val])\n",
        "\n",
        "    return np.array(final_list_train), encoded_train, np.array(final_list_test), encoded_test, onehot_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9_A_w57pj4Y",
        "outputId": "1b0f5289-040b-49d1-aad4-1e0f4f012986"
      },
      "outputs": [],
      "source": [
        "seq_len = 300\n",
        "X_train, y_train, X_test, y_test, vocab = tokenize(X_train,\n",
        "                                                   y_train,\n",
        "                                                   X_test,\n",
        "                                                   y_test,\n",
        "                                                   seq_len)\n",
        "\n",
        "len(X_train), len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J61TtmqcuVTc",
        "outputId": "3ff460d7-accf-49a7-d53d-c3a95db79657"
      },
      "outputs": [],
      "source": [
        "X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_XIMBMRwZWN"
      },
      "source": [
        "## 4. Preparing the data using DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tudpNZvOwq07"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T5g7K3Iub9F",
        "outputId": "c3e40d7e-ea5b-4688-ec33-30453b19de01"
      },
      "outputs": [],
      "source": [
        "# Create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(X_train),\n",
        "                           torch.from_numpy(y_train))\n",
        "valid_data = TensorDataset(torch.from_numpy(X_test),\n",
        "                           torch.from_numpy(y_test))\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# Create train and valid dataloader\n",
        "train_dataloader = DataLoader(train_data,\n",
        "                              shuffle=True,\n",
        "                              batch_size=BATCH_SIZE)\n",
        "valid_dataloader = DataLoader(valid_data,\n",
        "                              shuffle=True,\n",
        "                              batch_size=BATCH_SIZE)\n",
        "\n",
        "len(train_dataloader), len(valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5rE9aJsxfbg",
        "outputId": "f93e2d00-832f-4bbc-973f-6306f1723942"
      },
      "outputs": [],
      "source": [
        "# Sampling one bacth from train_dataloader\n",
        "sample_data = iter(train_dataloader)\n",
        "sample_x, sample_y = next(sample_data)\n",
        "\n",
        "print('Sample input size:', sample_x.size())     # batch_size, seq_length\n",
        "print('Sample input:', sample_x)\n",
        "print('Sample output:', sample_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfhrHm3tx9wA"
      },
      "source": [
        "## 5. Building a LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyzct5DCRx8h"
      },
      "source": [
        "<img src = \"assets/LSTM-Model.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCdSBQfeyNQs",
        "outputId": "4c39fa95-421a-45d0-d8b7-fe52e4d3cad0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Setting device agnostic code\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulOsDj01Srqx"
      },
      "source": [
        "Here's a breakdown of how each component of the `SentimentLSTM` class processes the tokenized and padded array of text:\n",
        "\n",
        "### Embedding Layer\n",
        "The embedding layer (`self.embedding`) takes the tokenized and padded array of text as input. It converts each token (represented as an integer) into a dense vector of fixed size (`embedding_dim`). This dense representation captures semantic similarities between words. The input shape to the embedding layer is `(batch_size, sequence_length)`, where `batch_size` is the number of sequences in the batch and `sequence_length` is the length of each sequence.\n",
        "\n",
        "### LSTM Layer\n",
        "The LSTM layer (`self.lstm`) processes the embedded sequences over time. It takes the embedded input sequences and maintains hidden states to capture long-term dependencies. The LSTM layer consists of multiple LSTM units (determined by `no_layers`). Each LSTM unit processes one time step of the input sequence at a time. The input shape to the LSTM layer is `(batch_size, sequence_length, embedding_dim)`.\n",
        "\n",
        "### Dropout Layer\n",
        "The dropout layer (`self.dropout`) is applied after the LSTM layer. It helps prevent overfitting by randomly setting a fraction of input units to zero during training. This regularization technique improves the model's generalization performance. The input shape to the dropout layer is `(batch_size, sequence_length, hidden_dim)`.\n",
        "\n",
        "### Fully Connected (Linear) Layer\n",
        "The fully connected layer (`self.fc`) takes the output from the LSTM layer and transforms it into the desired output dimension (`output_dim`). It applies a linear transformation to the input data, followed by an optional bias term. The input shape to the fully connected layer is `(batch_size * sequence_length, hidden_dim)`.\n",
        "\n",
        "### Sigmoid Activation Function\n",
        "The sigmoid activation function (`self.sig`) is applied to the output of the fully connected layer. It squashes the output values to the range [0, 1], interpreting them as probabilities. In sentiment analysis, this allows the model to predict the probability of a given input belonging to a particular sentiment class (e.g., positive or negative). The output shape after applying the sigmoid function is `(batch_size * sequence_length)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UAHIy-TMxzOJ"
      },
      "outputs": [],
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "  def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, output_dim = 1):\n",
        "    super(SentimentLSTM, self).__init__()\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.no_layers = no_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    # Embedding and LSTM layers\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # LSTM Layer\n",
        "    self.lstm = nn.LSTM(input_size = embedding_dim,\n",
        "                        hidden_size = self.hidden_dim,\n",
        "                        num_layers = no_layers,\n",
        "                        batch_first = True)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.3)                    # Dropout Layer\n",
        "    self.fc = nn.Linear(self.hidden_dim, output_dim)  # Linear Layer\n",
        "    self.sig = nn.Sigmoid()                           # Sigmoid Activation Function\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    \"\"\" Forward pass \"\"\"\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # Embeddings and lstm_out\n",
        "    embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "    # print(embeds.shape)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "    out = self.dropout(lstm_out)            # Dropout\n",
        "    out = self.fc(out)                      # Fully connected layer\n",
        "\n",
        "    sig_out = self.sig(out)                 # sigmoid function\n",
        "    sig_out = sig_out.view(batch_size, -1)  # Reshape to be batch_size first\n",
        "    sig_out = sig_out[:, -1]                # get last batch of labels\n",
        "    return sig_out, hidden                  # return last sigmoid output and hidden state\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    \"\"\" Initializes hidden state \"\"\"\n",
        "    # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "    # Initialized to zero, for hidden state and cell state of LSTM\n",
        "    hidden_state = torch.zeros((self.no_layers,\n",
        "                                batch_size,\n",
        "                                self.hidden_dim)).to(device)\n",
        "    # print(batch_size)\n",
        "    cell_state = torch.zeros((self.no_layers,\n",
        "                              batch_size,\n",
        "                              self.hidden_dim)).to(device)\n",
        "    hidden = (hidden_state, cell_state)\n",
        "    # print(\"Hidden\", hidden_state.size())\n",
        "    # print(\"Cell\", cell_state.size())\n",
        "    return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImnTLXQY0K_A",
        "outputId": "e8a56687-b12c-4e77-b213-8bd2620bd9fb"
      },
      "outputs": [],
      "source": [
        "model = SentimentLSTM(no_layers = 2,               # LSTM doesn't perform quite well beyound 2 layers\n",
        "                      vocab_size = len(vocab) + 1, # 1 extra for padding\n",
        "                      hidden_dim = 256,\n",
        "                      embedding_dim = 64)\n",
        "\n",
        "# Pass model to device 'cuda' if selected\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVe0MYCA4w5L"
      },
      "source": [
        "## 6. Create Train and Validation Loop Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "B6LbxATe7MNN"
      },
      "outputs": [],
      "source": [
        "# Function to predict accuracy\n",
        "def acc(pred, label):\n",
        "  pred = torch.round(pred.squeeze())\n",
        "  return torch.sum(pred == label.squeeze()).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "m6IE2Aep37PS"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs, clip, device):\n",
        "  valid_loss_min = np.Inf\n",
        "  epoch_tr_loss, epoch_vl_loss = [], []\n",
        "  epoch_tr_acc, epoch_vl_acc = [], []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    # Initialize hidden state\n",
        "    h = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      # Creating new variables for the hidden state to prevent backpropagation\n",
        "      # through the entire training history\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output, h = model(inputs, h)\n",
        "      loss = criterion(output.squeeze(), labels.float())\n",
        "      loss.backward()\n",
        "\n",
        "      train_losses.append(loss.item())\n",
        "      accuracy = acc(output, labels)\n",
        "      train_acc += accuracy\n",
        "      clip_grad_norm_(model.parameters(), clip)\n",
        "      optimizer.step()\n",
        "\n",
        "    val_h = model.init_hidden(BATCH_SIZE)\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in valid_loader:\n",
        "      val_h = tuple([each.data for each in val_h])\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      output, val_h = model(inputs, val_h)\n",
        "      val_loss = criterion(output.squeeze(), labels.float())\n",
        "      val_losses.append(val_loss.item())\n",
        "      accuracy = acc(output, labels)\n",
        "      val_acc += accuracy\n",
        "\n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)\n",
        "    epoch_train_acc = train_acc / len(train_loader.dataset)\n",
        "    epoch_val_acc = val_acc / len(valid_loader.dataset)\n",
        "\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "\n",
        "    print(f'Epoch: {epoch+1} -- train_loss: {epoch_train_loss:.4f} -- train_acc: {epoch_train_acc*100:.2f}% -- val_loss: {epoch_val_loss:.4f} -- val_acc: {epoch_val_acc*100:.2f}%')\n",
        "\n",
        "    # Saving model on each epoch except when valid_loss_for_next_epoch > valid_loss_for_prev_epoch\n",
        "    if epoch_val_loss <= valid_loss_min:\n",
        "      torch.save(model.state_dict(), 'sentiment_model.pth')\n",
        "      print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, epoch_val_loss))\n",
        "      valid_loss_min = epoch_val_loss\n",
        "    print(25*'==')\n",
        "\n",
        "  return epoch_tr_loss, epoch_vl_loss, epoch_tr_acc, epoch_vl_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2oybwIQ-Pwd"
      },
      "source": [
        "## 7. Train and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eW7H3-ln5zm5"
      },
      "outputs": [],
      "source": [
        "# Loss function and Optimizer\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmihVKQY561l",
        "outputId": "84db9608-555d-4ac3-d05a-85e4f0cdda1b"
      },
      "outputs": [],
      "source": [
        "train_losses, val_losses, train_accs, val_accs = train_model(model = model,\n",
        "                                                             train_loader = train_dataloader,\n",
        "                                                             valid_loader = valid_dataloader,\n",
        "                                                             criterion = loss_fn,\n",
        "                                                             optimizer = optimizer,\n",
        "                                                             epochs = 5,\n",
        "                                                             clip = 5,\n",
        "                                                             device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niBrXV4v-asL"
      },
      "source": [
        "## 8. Plot Loss and Accuracy Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "5VM0Y5HJ59mV",
        "outputId": "720bd2e0-34e4-4bc6-ff1b-d6a0fa4be5af"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (20, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train loss')\n",
        "plt.plot(val_losses, label='Validation loss')\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Acc')\n",
        "plt.plot(val_accs, label='Validation Acc')\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UrCIXYQk3Y-"
      },
      "source": [
        "## 9. Load Model to `cpu`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63_PktMxk9li",
        "outputId": "358e9e5f-4e4c-44f2-b6e8-b503f2c17c1e"
      },
      "outputs": [],
      "source": [
        "# Load model configs\n",
        "model = SentimentLSTM(no_layers = 2,\n",
        "                      vocab_size = len(vocab) + 1,\n",
        "                      hidden_dim = 256,\n",
        "                      embedding_dim = 64)\n",
        "\n",
        "# Load the model state dict\n",
        "model.load_state_dict(torch.load(\"sentiment_model.pth\", map_location=device))\n",
        "\n",
        "# Move the model to CPU\n",
        "model.to('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_-SrL4xlI8i"
      },
      "source": [
        "## 10. Make Prediction using Loaded Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "SQ5KVW-pWuIn"
      },
      "outputs": [],
      "source": [
        "def padding_(seq, max_length):\n",
        "    \"\"\"Pad sequences to the same length\"\"\"\n",
        "    if len(seq) >= max_length:\n",
        "        return seq[:max_length]\n",
        "    else:\n",
        "        padded = np.zeros((seq.shape[0], max_length), dtype=int)\n",
        "        padded[:, :seq.shape[1]] = seq\n",
        "        return padded\n",
        "\n",
        "def predict_text(text):\n",
        "    word_seq = np.array([vocab[preprocess_string(word)] for word in text.split()\n",
        "                         if preprocess_string(word) in vocab.keys()])\n",
        "    word_seq = np.expand_dims(word_seq, axis=0)\n",
        "    pad = torch.from_numpy(padding_(word_seq, 300))\n",
        "    inputs = pad.to('cpu')  # Move inputs to CPU\n",
        "    batch_size = 1\n",
        "    h = model.init_hidden(batch_size)\n",
        "    # Move the initial hidden state to CPU\n",
        "    h = tuple([each.to('cpu') for each in h])\n",
        "    # Set to evaluation mode\n",
        "    with torch.inference_mode():\n",
        "        output, h = model(inputs, h)\n",
        "    return output.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAZKKQ3R-jnm",
        "outputId": "acf87a5a-091e-474b-c38e-7d07bb44c014"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Randomly select 5 index from dataset to predict\n",
        "for _ in range(5):\n",
        "    index = random.randint(0, 550000)\n",
        "    review = df['review'][index]\n",
        "    actual_sentiment = df['sentiment'][index]\n",
        "\n",
        "    print(\"Review:\")\n",
        "    print(review)\n",
        "    print('='*70)\n",
        "    print(f'Actual sentiment: {actual_sentiment}')\n",
        "    print('='*70)\n",
        "    pro = predict_text(review)\n",
        "    predicted_sentiment = \"positive\" if pro > 0.45 else \"negative\"\n",
        "    pro = (1 - pro) if predicted_sentiment == \"negative\" else pro\n",
        "\n",
        "    # Color-coding output based on prediction matching actual sentiment\n",
        "    if actual_sentiment == predicted_sentiment:\n",
        "        color = '\\033[92m'  # Green color for matching sentiment\n",
        "    else:\n",
        "        color = '\\033[91m'  # Red color for mismatching sentiment\n",
        "\n",
        "    print(f'Predicted sentiment: {color}{predicted_sentiment}\\033[0m with a probability of {pro * 100:.2f}%')\n",
        "    print('='*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "oKPYtFXnY7JR"
      },
      "outputs": [],
      "source": [
        "def predict_custom_text(custom_text):\n",
        "  pro = predict_text(custom_text)\n",
        "  status = \"positive\" if pro > 0.45 else \"negative\"\n",
        "  pro = (1 - pro) if status == \"negative\" else pro\n",
        "  print(f'Predicted: {status} \\nProbability {pro*100 :.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPhqzNtbZSGc",
        "outputId": "e2229874-4e04-452f-9e9d-a8fcdbc9dc5d"
      },
      "outputs": [],
      "source": [
        "# Custom Negative Text\n",
        "custom_text = \"I can't even begin to express my sheer frustration and utter disappointment with this product! It's an absolute disaster from start to finish. \"\n",
        "predict_custom_text(custom_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMiIvFV7uJVB",
        "outputId": "7bfa8113-5955-4809-d51c-f0df2e22bf74"
      },
      "outputs": [],
      "source": [
        "# Custom Positive Text\n",
        "custom_text = \"I recently watched a movie that completely exceeded my expectations. From the captivating storyline to the stellar performances of the cast, every aspect of the film was a delight. The characters were well-developed and relatable, each with their own unique charm that drew me into their world. The dialogue was witty and engaging, keeping me entertained from start to finish. Additionally, the stunning visuals and masterful cinematography added depth and richness to the viewing experience. Overall, it was a cinematic masterpiece that left me feeling uplifted and inspired. I highly recommend this movie to anyone looking for a heartwarming and enjoyable film.\"\n",
        "predict_custom_text(custom_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lor3vocUX10r",
        "outputId": "63a8c97a-ff1e-42ca-ae9b-e7163647c384"
      },
      "outputs": [],
      "source": [
        "i = [123, 345, 456, 678, 88888]\n",
        "\n",
        "for i in range(len(i)):\n",
        "  custom_text = df[\"review\"][i]\n",
        "  print(\"\\nContext:\", df[\"review\"][i])\n",
        "  print(\"Actual:\", df[\"sentiment\"][i])\n",
        "  predict_custom_text(custom_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
